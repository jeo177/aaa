{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ahfft7Mc2Mea"
      },
      "source": [
        "### Image Classification Using DeiT\n",
        "Image classification is the canonical computer vision task of determining if an image contains a specific object, feature, or activity.\n",
        "\n",
        "This notebook demonstrates training an image classifi using Vision Transformers. This demonstration uses Data-efficient Image Transformers DeiT pretrained on ImageNet for image classification.\n",
        "\n",
        "\n",
        "The DeiT model was proposed in [Training data-efficient image transformers & distillation through attention](https://arxiv.org/abs/2012.12877) by Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Hervé Jégou. The Vision Transformer (ViT) introduced in Dosovitskiy et al., 2020 has shown that one can match or even outperform existing convolutional neural networks using a Transformer encoder (BERT-like). However, the ViT models introduced in that paper required training on expensive infrastructure for multiple weeks, using external data. DeiT (data-efficient image transformers) are more efficiently trained transformers for image classification, requiring far less data and far less computing resources compared to the original ViT models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neyZKhsj6cqF"
      },
      "source": [
        "We install roboflow first.\n",
        "Also, we will be saving the checkpoints in the /content/checkpoints. So let's create that folder as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0P0TpdAtFrq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "!pip3 install roboflow\n",
        "os.mkdir('/content/checkpoints')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAqe4Ux56phE"
      },
      "source": [
        "## Import section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TRqrePOs319g"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import requests\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "from roboflow import Roboflow\n",
        "from transformers import AutoImageProcessor, DeiTForImageClassification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oC3hoJF86thi"
      },
      "source": [
        "## Load the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIgyQi0q4Jke",
        "outputId": "418a38b5-3811-4d83-ecb5-d250fbcc6d1e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n",
            "Some weights of DeiTForImageClassification were not initialized from the model checkpoint at facebook/deit-base-distilled-patch16-224 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "image_processor = AutoImageProcessor.from_pretrained(\"facebook/deit-base-distilled-patch16-224\")\n",
        "model = DeiTForImageClassification.from_pretrained(\"facebook/deit-base-distilled-patch16-224\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXczPIvt60ow"
      },
      "source": [
        "## Vanilla inference\n",
        "We will do the inference on the downloaded model on a test image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2JRJcZa4SeC",
        "outputId": "96a443ec-68bd-43c7-d29f-ae8819fde807"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting inference on pretrained model...\n",
            "Predicted class: bucket, pail\n",
            "Completed inference on pretrained model!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Starting inference on pretrained model...\")\n",
        "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "inputs = image_processor(images=image, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "logits = outputs.logits\n",
        "# model predicts one of the 1000 ImageNet classes\n",
        "predicted_class_idx = logits.argmax(-1).item()\n",
        "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n",
        "print(\"Completed inference on pretrained model!\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItaN4Eof7tZb"
      },
      "source": [
        "## Load Data\n",
        "We will be using a roboflow dataset. I am demonstrating this training pipeline using Tumour-Classification-1 dataset but you can use any dataset that you like.\n",
        "\n",
        "We will also prepare train and val dataloaders in this section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEUC3Sgd7UHI",
        "outputId": "9b1b1333-9cf3-49a4-d009-8d31ebb53fed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Commencing data download from roboflow...\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Data download complete!\n",
            "\n",
            "Preparing dataloaders...\n",
            "classes:  ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
            "Dataloaders prepared!\n"
          ]
        }
      ],
      "source": [
        "print(\"Commencing data download from roboflow...\")\n",
        "rf = Roboflow(api_key=\"<YOUR_API_KEY>\")\n",
        "project = rf.workspace(\"brain-tumor-c6lzv\").project(\"tumor-classification-ufzoh\")\n",
        "dataset = project.version(1).download(\"folder\")\n",
        "print(\"Data download complete!\\n\")\n",
        "\n",
        "print(\"Preparing dataloaders...\")\n",
        "\n",
        "train_dir = './Tumor-Classification-1/train'\n",
        "val_dir = './Tumor-Classification-1/valid'\n",
        "\n",
        "train_dataset = datasets.ImageFolder(train_dir, transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.ToTensor()\n",
        "]))\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "counter = 0\n",
        "\n",
        "val_dataset = datasets.ImageFolder(val_dir, transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.ToTensor()\n",
        "]))\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "print(\"classes: \", train_dataset.classes)\n",
        "print(\"Dataloaders prepared!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQrLoHrF7JVf"
      },
      "source": [
        "## Training\n",
        "Note: I have set the epochs to only 5 for demonstration purpose. You may use the appropriate number of epochs to get the desired finetuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIxh_dJrsdUl"
      },
      "outputs": [],
      "source": [
        "\n",
        "class DeiT():\n",
        "\n",
        "    def train(self, train_loader, model, criterion, optimizer, epoch):\n",
        "        model.train()\n",
        "        losses = []\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        print(\"training:\")\n",
        "        for input, target in tqdm(train_loader):\n",
        "          target = target.cuda()\n",
        "          input_var = torch.autograd.Variable(input).cuda()\n",
        "          target_var = torch.autograd.Variable(target).cuda()\n",
        "\n",
        "          output = model(input_var)\n",
        "          _, preds = torch.max(output.logits, 1)\n",
        "          loss = criterion(output.logits.cuda(), target_var)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          losses.append(loss.detach().cpu().numpy())\n",
        "\n",
        "          running_loss += loss.detach().cpu().numpy() * input.size(0)\n",
        "          running_corrects += torch.sum(preds == target.data)\n",
        "\n",
        "        return running_loss, running_corrects\n",
        "\n",
        "    def validate(self, val_loader, model, criterion):\n",
        "\n",
        "        model.eval()\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "        print(\"validation:\")\n",
        "        for input, target in tqdm(val_loader):\n",
        "            target = target.cuda()\n",
        "            input_var = torch.autograd.Variable(input, volatile=True).cuda()\n",
        "            target_var = torch.autograd.Variable(target, volatile=True).cuda()\n",
        "\n",
        "            output = model(input_var)\n",
        "            _, preds = torch.max(output.logits, 1)\n",
        "            loss = criterion(output.logits.cuda(), target_var)\n",
        "\n",
        "            running_loss += loss.detach().cpu().numpy() * input.size(0)\n",
        "            # print(running_loss)\n",
        "            running_corrects += torch.sum(preds == target.data)\n",
        "\n",
        "        return running_loss, running_corrects\n",
        "\n",
        "    def run(self):\n",
        "\n",
        "        model.classifier = nn.Linear(model.classifier.in_features, 4)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr=0.005)\n",
        "        epochs = 5\n",
        "        model.cuda()\n",
        "\n",
        "        print(\"Commencing training epochs...\")\n",
        "        for epoch in range(epochs):\n",
        "          print(\"\\nEpoch: \", epoch)\n",
        "\n",
        "          # train for one epoch\n",
        "          train_loss, train_acc = self.train(train_loader, model, criterion, optimizer, epoch)\n",
        "          epoch_train_loss = train_loss / len(train_loader)\n",
        "          epoch_train_acc = train_acc.double() / len(train_loader)\n",
        "\n",
        "          # evaluate on validation set\n",
        "          val_loss, val_acc = self.validate(val_loader, model, criterion)\n",
        "          epoch_val_loss = val_loss / len(val_loader)\n",
        "          epoch_val_acc = val_acc.double() / len(val_loader)\n",
        "\n",
        "          print(f'train_acc: {epoch_train_acc:.4f} train_loss: {epoch_train_loss:.4f} val_acc: {epoch_val_acc:.4f} val_loss: {epoch_val_loss:.4f}')\n",
        "          PATH = '/content/checkpoints/epoch{epoch}_{loss:.2f}.pth'.format(epoch=epoch, loss=epoch_train_loss)\n",
        "          torch.save(model, PATH)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    deit = DeiT()\n",
        "    deit.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hfsRq1L8XNI"
      },
      "source": [
        "## Testing\n",
        "We will use the test data from the downloaded roboflow dataset.\n",
        "Note: For the sake of demonstration, I have used the 4th checkpoint. You can use the best model for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MvJsBYyTa_o"
      },
      "outputs": [],
      "source": [
        "# inference\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "EVAL_BATCH = 1\n",
        "\n",
        "test_dir = '/content/Tumor-Classification-1/test'\n",
        "test_dataset = datasets.ImageFolder(test_dir, transforms.Compose([\n",
        "            transforms.Resize(224),\n",
        "            transforms.ToTensor()\n",
        "        ]))\n",
        "test_loader  = torch.utils.data.DataLoader(test_dataset, batch_size=EVAL_BATCH, shuffle=True, num_workers=4)\n",
        "\n",
        "model = torch.load('/content/checkpoints/epoch4_0.37.pth')\n",
        "model.eval()\n",
        "\n",
        "running_loss = 0.0\n",
        "running_corrects = 0\n",
        "classes = test_dataset.classes\n",
        "print(\"classes: \", classes)\n",
        "print(\"validation:\")\n",
        "with torch.no_grad():\n",
        "  for input, target in test_loader:\n",
        "      target = target.cuda()\n",
        "      input_var = torch.autograd.Variable(input, volatile=True).cuda()\n",
        "      target_var = torch.autograd.Variable(target, volatile=True).cuda()\n",
        "\n",
        "      output = model(input_var)\n",
        "      probabilities = torch.softmax(output.logits,dim=1)\n",
        "      pred = classes[torch.argmax(probabilities).detach().cpu().numpy()]\n",
        "      running_corrects += torch.sum(torch.tensor(pred == classes[target.detach().cpu().numpy()[0]]))\n",
        "\n",
        "      print(\"PREDICTED: \", pred)\n",
        "      print(\"TARGET: \", classes[target.detach().cpu().numpy()[0]])\n",
        "      print(\"\\n\\n\")\n",
        "\n",
        "\n",
        "  test_acc = running_corrects.double() / len(test_loader)\n",
        "  print(\"test accuracy: \", test_acc.detach().cpu().numpy())\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
